# Fine tune Distil BERT
## 2 Finetuning DistilBERT [43 points]
For this section, you will use a pretrained language model called [DistilBERT](https://arxiv.org/abs/1910.01108), a less hardware intensive version of the popular model [BERT](https://arxiv.org/abs/1810.04805).

Run the following code to load DistilBERT's custom tokenizer, along with the model itself (this will load the model weights too).