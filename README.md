# Fine tune Distil BERT
## 2 Finetuning DistilBERT 
For this section, you will use a pretrained language model called [DistilBERT](https://arxiv.org/abs/1910.01108), a less hardware intensive version of the popular model [BERT](https://arxiv.org/abs/1810.04805).

Run the following code to load DistilBERT's custom tokenizer, along with the model itself (this will load the model weights too).

# Quick Start
Before running the project, add execute permission to the run script:
```bash
chmod +x run
./run
```
This will set up the environment and run the training pipeline.